{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as ssd\n",
    "import networkx as nx\n",
    "\n",
    "# Function to calculate the Jensen-Shannon distance\n",
    "def jensen_shannon_distance(p, q):\n",
    "    m = (p + q) / 2\n",
    "    return np.sqrt(0.5 * (ssd.jensenshannon(p, m) + ssd.jensenshannon(q, m)))\n",
    "\n",
    "# Function to calculate document similarity\n",
    "def calculate_document_similarity(doc1, doc2, threshold=0.05):\n",
    "    # Tokenize and count words in each document\n",
    "    tokenized_doc1 = doc1.lower().split()\n",
    "    tokenized_doc2 = doc2.lower().split()\n",
    "    word_counts_doc1 = pd.Series(tokenized_doc1).value_counts()\n",
    "    word_counts_doc2 = pd.Series(tokenized_doc2).value_counts()\n",
    "    \n",
    "    # Create a set of all unique words in both documents\n",
    "    word_set = set(word_counts_doc1.index) | set(word_counts_doc2.index)\n",
    "    \n",
    "    # Convert word counts to probability distributions\n",
    "    p = np.zeros(len(word_set))\n",
    "    q = np.zeros(len(word_set))\n",
    "    \n",
    "    for i, word in enumerate(word_set):\n",
    "        p[i] = word_counts_doc1.get(word, 0) / len(tokenized_doc1)\n",
    "        q[i] = word_counts_doc2.get(word, 0) / len(tokenized_doc2)\n",
    "    \n",
    "    # Calculate the Jensen-Shannon distance\n",
    "    dist = jensen_shannon_distance(p, q)\n",
    "    \n",
    "    # Check if distance is less than threshold\n",
    "    if dist <= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to create a graph from a list of documents\n",
    "def create_document_graph(docs, threshold=0.05):\n",
    "    n = len(docs)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    for i in range(n):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if calculate_document_similarity(docs[i], docs[j], threshold=threshold):\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Function to cluster documents using the Louvain method\n",
    "def cluster_documents_louvain(docs, threshold=0.05):\n",
    "    # Create a graph from the documents\n",
    "    G = create_document_graph(docs, threshold=threshold)\n",
    "    \n",
    "    # Cluster the graph using the Louvain method\n",
    "    partition = nx.community.modularity_max.greedy_modularity_communities(G)\n",
    "    \n",
    "    # Convert the partition to a list of lists\n",
    "    clusters = []\n",
    "    for part in partition:\n",
    "        clusters.append(list(part))\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Function to cluster documents using the Girvan-Newman method\n",
    "def cluster_documents_girvan_newman(docs, threshold=0.05):\n",
    "    # Create a graph from the documents\n",
    "    G = create_document_graph(docs, threshold=threshold)\n",
    "    \n",
    "    # Cluster the graph using the Girvan-Newman method\n",
    "    partition = nx.community.girvan_newman(G)\n",
    "    clusters = []\n",
    "    \n",
    "    # Get the clustering at each level of the dendrogram\n",
    "    for part in partition:\n",
    "        if len(part) > 1:\n",
    "            clusters.append([list(c) for c in part])\n",
    "    \n",
    "    return clusters\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
